{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 26)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('2-Span_Dataset_190914-1.xlsx', header = None, skiprows=2, nrows = 10002-2)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1         2          3          4   5           6   \\\n",
      "12     1  8.036693  6.621703  59.930830  13.302514   0  -11.243810   \n",
      "15     1  3.314051  6.650936  69.294040   5.946864   0   -6.415482   \n",
      "16     1  4.474462  6.487012  72.919013  10.609219   0   -1.502025   \n",
      "17     1  8.682127  3.167319  61.665732   5.483042   0   71.456656   \n",
      "18     1  8.072093  8.133456  81.592774  14.610197   0  -20.260882   \n",
      "...   ..       ...       ...        ...        ...  ..         ...   \n",
      "9980   1  4.028240  6.093211  56.443040   9.243475   0   -2.346499   \n",
      "9985   1  7.771782  3.427451  91.787825   3.258468   0  112.184813   \n",
      "9995   1  3.970696  5.790132  61.591161   8.935587   0   -2.305294   \n",
      "9996   1  5.529386  4.302831  52.422779   0.581853   0   12.450545   \n",
      "9999   1  6.798161  9.714735  58.706105   8.886615   0  -23.122217   \n",
      "\n",
      "              7           8           9   ...         16          17  \\\n",
      "12    -21.806177  -31.005658  -38.160810  ...  20.595110   43.215922   \n",
      "15    -12.442147  -17.691178  -21.773757  ...  32.347814   71.399801   \n",
      "16     -2.913018   -4.141948   -5.097782  ...   5.215931   11.505363   \n",
      "17    137.359013  192.152769  230.283625  ... -12.774341  -21.514680   \n",
      "18    -39.293833  -55.870918  -68.764207  ...  46.406706   98.866807   \n",
      "...          ...         ...         ...  ...        ...         ...   \n",
      "9980   -4.550786   -6.470648   -7.963875  ...   8.559154   18.942861   \n",
      "9985  212.456228  288.900850  329.605281  ... -17.182215  -28.938468   \n",
      "9995   -4.470874   -6.357024   -7.824030  ...   8.056672   17.764726   \n",
      "9996   21.053156   25.490693   26.478175  ...  -2.685328   -4.522658   \n",
      "9999  -44.843087  -63.761265  -78.475403  ...  76.358187  153.119878   \n",
      "\n",
      "              18          19          20          21          22          23  \\\n",
      "12     65.262050   84.133104   97.228699  101.948445   95.691955   75.859031   \n",
      "15    108.200104  133.793009  141.524969  132.876496  111.123675   79.542595   \n",
      "16     18.028414   23.945206   28.415861   30.600497   29.659237   24.752200   \n",
      "17    -26.669239  -28.686240  -28.013906  -25.100460  -20.394124  -14.343120   \n",
      "18    150.966970  196.293865  228.434158  240.974519  227.501615  181.602115   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "9980   29.721817   39.466719   46.748265   50.137152   48.204076   39.519736   \n",
      "9985  -35.871642  -38.584623  -37.680296  -33.761545  -27.431256  -19.292312   \n",
      "9995   27.804661   36.856971   43.602154   46.720706   44.893124   36.799903   \n",
      "9996   -5.606212   -6.030211   -5.888878   -5.276435   -4.287103   -3.015106   \n",
      "9999  205.379578  229.091159  228.317423  207.139256  169.637544  119.893173   \n",
      "\n",
      "              24            25  \n",
      "12     41.849442  2.434284e-13  \n",
      "15     41.409341  1.990874e-13  \n",
      "16     15.039507  1.793708e-12  \n",
      "17     -7.395671 -9.002062e-15  \n",
      "18    100.764649  4.099577e-13  \n",
      "...          ...           ...  \n",
      "9980   22.764272 -4.385626e-13  \n",
      "9985   -9.947598  1.948280e-14  \n",
      "9995   21.213485  2.336351e-13  \n",
      "9996   -1.554664  0.000000e+00  \n",
      "9999   61.987030  8.217995e-14  \n",
      "\n",
      "[1626 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "data= data.loc[data[0]==1]\n",
    "data= data.loc[data[3]>=20]\n",
    "data= data.loc[data[1]>=3]\n",
    "data= data.loc[data[2]>=3]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data.shape[0]\n",
    "\n",
    "X = np.zeros((N,21,1))\n",
    "X2 = np.zeros((N,3,1))\n",
    "Y = np.zeros((N,21))\n",
    "\n",
    "\n",
    "for ii in range(0,N):\n",
    "    a = data.iloc[ii,4]\n",
    "    l1 = data.iloc[ii,1]\n",
    "    l2 = data.iloc[ii,2]\n",
    "    P = data.iloc[ii,3]\n",
    "    \n",
    "    if a<=l1:\n",
    "        X[ii,int(np.round(a/(l1/10))),0] = 1\n",
    "    if a>l1:\n",
    "        X[ii,int(np.round(10+(a-l1)/(l2/10))),0] = 1\n",
    "    \n",
    "    X2[ii,0,0]=l1\n",
    "    X2[ii,1,0]=l2\n",
    "    X2[ii,2,0]=P\n",
    "    #print(X[ii,:,0])\n",
    "    Y[ii,:] = data.iloc[ii,5:26]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Tensor(\"dense_1/Relu:0\", shape=(?, 21), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "WARNING:tensorflow:From c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\nuno\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1626 samples, validate on 1626 samples\n",
      "Epoch 1/500\n",
      "1626/1626 [==============================] - 1s 532us/step - loss: 8219.4286 - val_loss: 7965.0080\n",
      "Epoch 2/500\n",
      "1626/1626 [==============================] - 0s 131us/step - loss: 7145.3668 - val_loss: 5876.5263\n",
      "Epoch 3/500\n",
      "1626/1626 [==============================] - 0s 131us/step - loss: 4124.6597 - val_loss: 3093.8600\n",
      "Epoch 4/500\n",
      "1626/1626 [==============================] - 0s 142us/step - loss: 3015.1948 - val_loss: 3125.3888\n",
      "Epoch 5/500\n",
      "1626/1626 [==============================] - 0s 139us/step - loss: 2866.8027 - val_loss: 2652.1950\n",
      "Epoch 6/500\n",
      "1626/1626 [==============================] - 0s 139us/step - loss: 2678.1417 - val_loss: 2783.2502\n",
      "Epoch 7/500\n",
      "1626/1626 [==============================] - 0s 134us/step - loss: 2663.4141 - val_loss: 2532.7626\n",
      "Epoch 8/500\n",
      "1626/1626 [==============================] - 0s 144us/step - loss: 2558.5851 - val_loss: 2661.3957\n",
      "Epoch 9/500\n",
      "1626/1626 [==============================] - 0s 141us/step - loss: 2567.5570 - val_loss: 2628.4463\n",
      "Epoch 10/500\n",
      "1626/1626 [==============================] - 0s 142us/step - loss: 2490.8445 - val_loss: 2438.3243\n",
      "Epoch 11/500\n",
      "1626/1626 [==============================] - 0s 194us/step - loss: 2438.5346 - val_loss: 2337.0452\n",
      "Epoch 12/500\n",
      "1626/1626 [==============================] - 0s 172us/step - loss: 2419.2659 - val_loss: 2361.5896\n",
      "Epoch 13/500\n",
      "1626/1626 [==============================] - 0s 180us/step - loss: 2376.9262 - val_loss: 2242.0007\n",
      "Epoch 14/500\n",
      "1626/1626 [==============================] - 0s 167us/step - loss: 2335.9383 - val_loss: 2255.5444\n",
      "Epoch 15/500\n",
      "1626/1626 [==============================] - 0s 146us/step - loss: 2364.2078 - val_loss: 2195.2544\n",
      "Epoch 16/500\n",
      "1626/1626 [==============================] - 0s 142us/step - loss: 2180.6234 - val_loss: 2229.2882\n",
      "Epoch 17/500\n",
      "1626/1626 [==============================] - 0s 136us/step - loss: 2160.5138 - val_loss: 2032.7859\n",
      "Epoch 18/500\n",
      "1626/1626 [==============================] - 0s 136us/step - loss: 2053.8020 - val_loss: 1949.0364\n",
      "Epoch 19/500\n",
      "1626/1626 [==============================] - 0s 137us/step - loss: 1968.2886 - val_loss: 1847.3886\n",
      "Epoch 20/500\n",
      "1626/1626 [==============================] - 0s 134us/step - loss: 1849.1334 - val_loss: 1741.0766\n",
      "Epoch 21/500\n",
      "1626/1626 [==============================] - 0s 144us/step - loss: 1821.9551 - val_loss: 1640.5986\n",
      "Epoch 22/500\n",
      "1626/1626 [==============================] - 0s 145us/step - loss: 1635.8010 - val_loss: 1598.6592\n",
      "Epoch 23/500\n",
      "1626/1626 [==============================] - 0s 155us/step - loss: 1526.4054 - val_loss: 1474.4641\n",
      "Epoch 24/500\n",
      "1626/1626 [==============================] - 0s 152us/step - loss: 1467.3933 - val_loss: 1350.0587\n",
      "Epoch 25/500\n",
      "1626/1626 [==============================] - 0s 150us/step - loss: 1363.9363 - val_loss: 1416.3617\n",
      "Epoch 26/500\n",
      "1626/1626 [==============================] - 0s 146us/step - loss: 1355.2635 - val_loss: 1295.9698\n",
      "Epoch 27/500\n",
      "1626/1626 [==============================] - 0s 134us/step - loss: 1284.4050 - val_loss: 1226.0243\n",
      "Epoch 28/500\n",
      "1626/1626 [==============================] - 0s 131us/step - loss: 1254.5348 - val_loss: 1213.0314\n",
      "Epoch 29/500\n",
      "1626/1626 [==============================] - 0s 149us/step - loss: 1243.5688 - val_loss: 1186.0678\n",
      "Epoch 30/500\n",
      "1626/1626 [==============================] - 0s 156us/step - loss: 1243.5294 - val_loss: 1132.1482\n",
      "Epoch 31/500\n",
      "1626/1626 [==============================] - 0s 160us/step - loss: 1181.8436 - val_loss: 1133.2717\n",
      "Epoch 32/500\n",
      "1626/1626 [==============================] - 0s 159us/step - loss: 1300.0637 - val_loss: 1403.3664\n",
      "Epoch 33/500\n",
      "1626/1626 [==============================] - 0s 164us/step - loss: 1181.4421 - val_loss: 1141.5234\n",
      "Epoch 34/500\n",
      "1626/1626 [==============================] - 0s 171us/step - loss: 1282.2468 - val_loss: 1541.6508\n",
      "Epoch 35/500\n",
      "1626/1626 [==============================] - 0s 151us/step - loss: 1190.5023 - val_loss: 1154.9969\n",
      "Epoch 36/500\n",
      "1626/1626 [==============================] - 0s 148us/step - loss: 1150.3092 - val_loss: 1073.4381\n",
      "Epoch 37/500\n",
      "1626/1626 [==============================] - 0s 132us/step - loss: 1185.3569 - val_loss: 1200.4022\n",
      "Epoch 38/500\n",
      "1626/1626 [==============================] - 0s 128us/step - loss: 1276.9317 - val_loss: 1310.3214\n",
      "Epoch 39/500\n",
      "1626/1626 [==============================] - 0s 126us/step - loss: 1160.9430 - val_loss: 1104.4674\n",
      "Epoch 40/500\n",
      "1626/1626 [==============================] - 0s 138us/step - loss: 1208.5881 - val_loss: 1092.2560\n",
      "Epoch 41/500\n",
      "1626/1626 [==============================] - 0s 134us/step - loss: 1224.7813 - val_loss: 1292.1627\n",
      "Epoch 42/500\n",
      "1626/1626 [==============================] - 0s 126us/step - loss: 1184.9910 - val_loss: 1105.5681\n",
      "Epoch 43/500\n",
      "1626/1626 [==============================] - 0s 126us/step - loss: 1106.3082 - val_loss: 1050.5446\n",
      "Epoch 44/500\n",
      "1626/1626 [==============================] - 0s 130us/step - loss: 1111.2651 - val_loss: 1157.8486\n",
      "Epoch 45/500\n",
      "1626/1626 [==============================] - 0s 132us/step - loss: 1128.0302 - val_loss: 1041.0024\n",
      "Epoch 46/500\n",
      "1626/1626 [==============================] - 0s 146us/step - loss: 1093.6932 - val_loss: 1074.9368\n",
      "Epoch 47/500\n",
      "1626/1626 [==============================] - 0s 145us/step - loss: 1117.2241 - val_loss: 1154.6063\n",
      "Epoch 48/500\n",
      "1626/1626 [==============================] - 0s 180us/step - loss: 1185.6624 - val_loss: 1125.5079\n",
      "Epoch 49/500\n",
      "1626/1626 [==============================] - 0s 167us/step - loss: 1084.0761 - val_loss: 1042.5068\n",
      "Epoch 50/500\n",
      "1626/1626 [==============================] - 0s 148us/step - loss: 1089.2811 - val_loss: 1408.8543\n",
      "Epoch 51/500\n",
      "1626/1626 [==============================] - 0s 128us/step - loss: 1199.9683 - val_loss: 1047.6562\n",
      "Epoch 52/500\n",
      "1626/1626 [==============================] - 0s 129us/step - loss: 1062.9331 - val_loss: 1045.0301\n",
      "Epoch 53/500\n",
      "1626/1626 [==============================] - 0s 127us/step - loss: 1090.6252 - val_loss: 1010.9872\n",
      "Epoch 54/500\n",
      "1626/1626 [==============================] - 0s 130us/step - loss: 1025.6273 - val_loss: 1111.1159\n",
      "Epoch 55/500\n",
      "1626/1626 [==============================] - 0s 128us/step - loss: 1080.6370 - val_loss: 1050.9767\n",
      "Epoch 56/500\n",
      "1626/1626 [==============================] - 0s 126us/step - loss: 1042.1275 - val_loss: 1209.4966\n",
      "Epoch 57/500\n",
      "1626/1626 [==============================] - 0s 130us/step - loss: 998.1889 - val_loss: 941.4039\n",
      "Epoch 58/500\n",
      "1626/1626 [==============================] - 0s 125us/step - loss: 1129.8461 - val_loss: 1054.8948\n",
      "Epoch 59/500\n",
      "1626/1626 [==============================] - 0s 125us/step - loss: 1015.0649 - val_loss: 1145.2787\n",
      "Epoch 60/500\n",
      "1626/1626 [==============================] - 0s 133us/step - loss: 1004.3978 - val_loss: 969.3333\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626/1626 [==============================] - 0s 129us/step - loss: 971.2392 - val_loss: 952.5083\n",
      "Epoch 62/500\n",
      "1626/1626 [==============================] - 0s 142us/step - loss: 957.3588 - val_loss: 897.6725\n",
      "Epoch 63/500\n",
      "1626/1626 [==============================] - 0s 138us/step - loss: 1008.2671 - val_loss: 925.9493\n",
      "Epoch 64/500\n",
      "1626/1626 [==============================] - 0s 153us/step - loss: 968.1413 - val_loss: 940.6568\n",
      "Epoch 65/500\n",
      " 544/1626 [=========>....................] - ETA: 0s - loss: 807.6934"
     ]
    }
   ],
   "source": [
    "import keras as krs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define two sets of inputs\n",
    "inputA = krs.Input(shape=(21,1))\n",
    "inputB = krs.Input(shape=(3,))\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Conv1D(128, kernel_size=1, activation=\"relu\")(inputA)\n",
    "x = Conv1D(64, kernel_size=1, activation=\"relu\")(x)\n",
    "x = Conv1D(32, kernel_size=1, activation=\"relu\")(x)\n",
    "x = Conv1D(24, kernel_size=1, activation=\"relu\")(x)\n",
    "x = Conv1D(21, kernel_size=1, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(21, activation=\"relu\")(x)\n",
    "x = krs.Model(inputs=inputA, outputs=x)\n",
    "\n",
    "print(x.output)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(3, activation=\"linear\")(inputB)\n",
    "y = krs.Model(inputs=inputB, outputs=y)\n",
    "\n",
    "print(y.output)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = krs.layers.concatenate([x.output, y.output])\n",
    "\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(64, activation=\"relu\")(combined)\n",
    "z = Dense(32, activation=\"relu\")(z)\n",
    "z = Dense(24, activation=\"relu\")(z)\n",
    "z = Dense(21, activation=\"linear\")(z)\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = krs.Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history1 = model.fit( [X,X2[:,:,0]], Y, validation_data=([X,X2[:,:,0]], Y), epochs=500, batch_size=32, verbose=1)  \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "plt.figure()\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history1.history['loss'],label = 'mlp')\n",
    "  \n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict([X,X2[:,:,0]])\n",
    "import time\n",
    "print(Y[0,:])\n",
    "\n",
    "fig = plt.figure()\n",
    "for ii in range(20):\n",
    "    print(X[ii,:,0])\n",
    "    plt.plot(Y[ii,:],label='orig')\n",
    "    plt.plot(y_pred1[ii,:],label='NN')\n",
    "    plt.plot(Y[ii,:]-y_pred1[ii,:],label='error')\n",
    "    \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    time.sleep(2) \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = np.sum(np.abs(y_pred1-Y)/N,axis=0)\n",
    "\n",
    "print(\"errors1\\n \", err1)\n",
    "\n",
    "plt.plot(err1,label='error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "time.sleep(2) \n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
